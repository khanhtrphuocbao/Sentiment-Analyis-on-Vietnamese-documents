{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBCbyUm8QAO9"
      },
      "source": [
        "# INSTALL LIBRARY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6NaiF9WW2mg",
        "outputId": "4fc5e16a-a3fb-40b4-ae59-a972443a3b71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting underthesea\n",
            "  Downloading underthesea-6.2.0-py3-none-any.whl (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from underthesea) (8.1.3)\n",
            "Collecting python-crfsuite>=0.9.6 (from underthesea)\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from underthesea) (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from underthesea) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from underthesea) (2.27.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.2.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.2.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from underthesea) (6.0)\n",
            "Collecting underthesea-core==1.0.0 (from underthesea)\n",
            "  Downloading underthesea_core-1.0.0-cp310-cp310-manylinux2010_x86_64.whl (599 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.6/599.6 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->underthesea) (2022.10.31)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (3.1.0)\n",
            "Installing collected packages: underthesea-core, python-crfsuite, underthesea\n",
            "Successfully installed python-crfsuite-0.9.9 underthesea-6.2.0 underthesea-core-1.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting vncorenlp\n",
            "  Downloading vncorenlp-1.0.3.tar.gz (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vncorenlp) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (3.4)\n",
            "Building wheels for collected packages: vncorenlp\n",
            "  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-py3-none-any.whl size=2645933 sha256=bd2ecd8f39e0b7ce2631593ab5868bd63ce4e571a281a892f16cdbc7e4874224\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/d9/b3/41f6c6b1ab758561fd4aab55dc0480b9d7a131c6aaa573a3fa\n",
            "Successfully built vncorenlp\n",
            "Installing collected packages: vncorenlp\n",
            "Successfully installed vncorenlp-1.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install underthesea\n",
        "!pip install transformers\n",
        "!pip install vncorenlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnZSU3u3WQNP"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import re\n",
        "# from underthesea import word_tokenize\n",
        "from keras.utils import to_categorical\n",
        "from transformers import AutoTokenizer\n",
        "from tensorflow.data import Dataset\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvSR0aYqWQNZ"
      },
      "outputs": [],
      "source": [
        "from vncorenlp import VnCoreNLP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtv9WGHCqB6E"
      },
      "source": [
        "# LOAD DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1pBDQ41qKmc"
      },
      "source": [
        "## PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57n408h6WQNQ"
      },
      "outputs": [],
      "source": [
        "class TextNormalize:\n",
        "    def __init__(self):\n",
        "        self.vowels_to_ids = {}\n",
        "        self.vowels_table = [\n",
        "            ['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a' ],\n",
        "            ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
        "            ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
        "            ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e' ],\n",
        "            ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
        "            ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i' ],\n",
        "            ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o' ],\n",
        "            ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'o'],\n",
        "            ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
        "            ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u' ],\n",
        "            ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
        "            ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y' ]\n",
        "        ]\n",
        "        pass\n",
        "\n",
        "    def createVowelsTable(self):\n",
        "        \"\"\"Create Vowels Table\"\"\"\n",
        "        for i in range(len(self.vowels_table)):\n",
        "            for j in range(len(self.vowels_table[i]) - 1):\n",
        "                self.vowels_to_ids[self.vowels_table[i][j]] = (i, j)\n",
        "\n",
        "    def IsValidVietnameseWord(self,word):\n",
        "        \"\"\"Nguyên âm chỉ có thể đứng chung với nguyên âm. Một từ không thể có 2 nguyên âm cách nhau bởi 1 phụ âm\"\"\"\n",
        "        chars = list(word)\n",
        "        #nguyen am\n",
        "        vowel_index = -1\n",
        "        for i in range(len(chars)):\n",
        "            idx_vowel_table = self.vowels_to_ids.get(chars[i],(-1,-1))[0]\n",
        "            if idx_vowel_table != -1:\n",
        "                if vowel_index == -1:\n",
        "                    vowel_index = i\n",
        "                else:\n",
        "                    if i - vowel_index != 1:\n",
        "                        return False\n",
        "                    vowel_index = i\n",
        "        return True\n",
        "\n",
        "    def WordStandardized(self,word):\n",
        "        \"\"\"Standardize Word\"\"\"\n",
        "        if not self.IsValidVietnameseWord(word):\n",
        "            return word\n",
        "\n",
        "        chars = list(word)\n",
        "        vowel_indexes = []\n",
        "\n",
        "        # tìm vị trí nguyên âm\n",
        "        qu_or_gi = False\n",
        "        thanh_dieu = 0\n",
        "        for i in range(len(chars)):\n",
        "            vowel_table_row, vowel_table_col = self.vowels_to_ids.get(chars[i],(-1,-1))\n",
        "            if vowel_table_row == -1 :\n",
        "                continue\n",
        "            # qu\n",
        "            if vowel_table_row == 9:\n",
        "                if i != 0 and chars[i-1] == 'q':\n",
        "                    chars[i] = 'u'\n",
        "                    qu_or_gi = True\n",
        "            # gi\n",
        "            elif vowel_table_row == 5:\n",
        "                if i != 0 and chars[i-1] == 'g':\n",
        "                    chars[i] = 'i'\n",
        "                    qu_or_gi = True\n",
        "\n",
        "            # có chứa thanh điệu\n",
        "            if vowel_table_col != 0:\n",
        "                thanh_dieu = vowel_table_col\n",
        "                chars[i] = self.vowels_table[vowel_table_row][0]\n",
        "\n",
        "            vowel_indexes.append(i)\n",
        "        # 1 nguyên âm\n",
        "        if len(vowel_indexes) == 1:\n",
        "            c = chars[vowel_indexes[0]]\n",
        "            chars[vowel_indexes[0]] = self.vowels_table[self.vowels_to_ids[c][0]][thanh_dieu]\n",
        "            return ''.join(chars)\n",
        "\n",
        "        for idx_vowel in vowel_indexes:\n",
        "            vowel_table_row, vowel_table_col = self.vowels_to_ids.get(chars[idx_vowel],(-1,-1))\n",
        "            #ê, ơ, ô\n",
        "            if vowel_table_row == 4 or vowel_table_row == 7 or vowel_table_row == 8:\n",
        "                c = chars[idx_vowel]\n",
        "                chars[idx_vowel] = self.vowels_table[self.vowels_to_ids[c][0]][thanh_dieu]\n",
        "                return ''.join(chars)\n",
        "\n",
        "            # kiểm tra qu và gi, 2-3 nguyên âm thì nguyên âm thứ 2 chứa dấu\n",
        "            if qu_or_gi:\n",
        "                if len(vowel_indexes) == 2 or len(vowel_indexes) == 3:\n",
        "                    c = chars[vowel_indexes[1]]\n",
        "                    chars[vowel_indexes[1]] = self.vowels_table[self.vowels_to_ids[c][0]][thanh_dieu]\n",
        "                return ''.join(chars)\n",
        "\n",
        "            # 2 nguyên âm\n",
        "            if len(vowel_indexes) == 2:\n",
        "                # âm cuối là nguyên âm\n",
        "                if vowel_indexes[-1] == len(chars) - 1:\n",
        "                    c = chars[vowel_indexes[0]]\n",
        "                    chars[vowel_indexes[0]] = self.vowels_table[self.vowels_to_ids[c][0]][thanh_dieu]\n",
        "                else:\n",
        "                    c = chars[vowel_indexes[-1]]\n",
        "                    chars[vowel_indexes[-1]] = self.vowels_table[self.vowels_to_ids[c][0]][thanh_dieu]\n",
        "                return ''.join(chars)\n",
        "\n",
        "            elif len(vowel_indexes) == 3:\n",
        "                # âm cuối là nguyên âm\n",
        "                if vowel_indexes[-1] == len(chars) - 1:\n",
        "                    c = chars[vowel_indexes[1]]\n",
        "                    chars[vowel_indexes[1]] = self.vowels_table[self.vowels_to_ids[c][0]][thanh_dieu]\n",
        "                else:\n",
        "                    c = chars[vowel_indexes[-1]]\n",
        "                    chars[vowel_indexes[-1]] = self.vowels_table[self.vowels_to_ids[c][0]][thanh_dieu]\n",
        "                return ''.join(chars)\n",
        "\n",
        "        return ''.join(chars)\n",
        "\n",
        "    def normalize(self,text):\n",
        "\n",
        "        #Chuyen sang viet thuong\n",
        "        text = text.lower()\n",
        "\n",
        "        # Rút gọn từ kéo dài\n",
        "        text = re.sub(r'(\\w)\\1+',r'\\1',text)\n",
        "\n",
        "        # xóa các emoji dư thừa\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                            \"]+\", flags=re.UNICODE)\n",
        "        text = emoji_pattern.sub(r'',text) # no emoji\n",
        "\n",
        "        text = text.split()\n",
        "        # chuẩn hóa thanh điệu\n",
        "        for i in range(len(text)):\n",
        "            text[i] = self.WordStandardized(text[i])\n",
        "\n",
        "        text = ' '.join(text)\n",
        "\n",
        "        # xóa space d\n",
        "        text = re.sub(r\"( )\\1+\",r'\\1',text)\n",
        "        text = re.sub(r\"[:)^@!`~%;?(\\+\\-\\'\\\"]+\",r'',text)\n",
        "\n",
        "        # remove hastag\n",
        "        text = re.sub(\"(@[A-Za-z0-9]+)|(#[0-9A-Za-z]+)\",\" \", text)\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzVY-ShuyESH"
      },
      "outputs": [],
      "source": [
        "def convert_unicode(text):\n",
        "  char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'\n",
        "  charutf8 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'\n",
        "  char1252 = char1252.split('|')\n",
        "  charutf8 = charutf8.split('|')\n",
        "\n",
        "  dic = {}\n",
        "  for i in range(len(char1252)): dic[char1252[i]] = charutf8[i]\n",
        "  return re.sub(\n",
        "      r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
        "      lambda x: dic[x.group()], text\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vB8aZGNWQNS"
      },
      "outputs": [],
      "source": [
        "class LoadData():\n",
        "    def __init__(self, file_path):\n",
        "        self.file_path = file_path\n",
        "\n",
        "        self.data = pd.read_json(file_path, lines = True)\n",
        "        # self.data = self.data.dropna()\n",
        "        self.X = []\n",
        "        self.y = []\n",
        "    def transform(self,x,label):\n",
        "        y = []\n",
        "        if len(label) == 0:\n",
        "          return x,np.array([\"O\" for i in range(len(x.split()))])\n",
        "\n",
        "        first_index = label[:,0].astype(int)\n",
        "        second_index = label[:,1].astype(int)\n",
        "        asp_cate_pola = label[:,2]\n",
        "        s = 0\n",
        "        a = \"\"\n",
        "        # chay tu s -> first, cap nhat s\n",
        "        for i in range(len(label)):\n",
        "\n",
        "            front = x[s:first_index[i]]\n",
        "            # print(first_index[i],second_index[i])\n",
        "            middle = x[first_index[i]:second_index[i]]\n",
        "            # print(x)\n",
        "            # print(middle)\n",
        "            s = second_index[i]\n",
        "\n",
        "            a += front + \" \" + middle + \" \"\n",
        "            y.extend([\"O\" for i in range(len(front.split()))])\n",
        "            y.extend([f\"B-{asp_cate_pola[i]}\" if j == 0 else f\"I-{asp_cate_pola[i]}\" for j in range(len(middle.split()))])\n",
        "\n",
        "        if s != len(x):\n",
        "            a+= x[s:]\n",
        "            y.extend([\"O\" for i in range(len(x[s:].split()))])\n",
        "\n",
        "        # print(a)\n",
        "        # for k, v in zip(a.split(),y):\n",
        "        #     print(k,\"=>\",v)\n",
        "        return a,np.array(y)\n",
        "\n",
        "    def ExtractAspectTermPosition(self,span_labels):\n",
        "        labels = []\n",
        "        # print(aspectTerms)\n",
        "        for ls in span_labels:\n",
        "            start = ls[0]\n",
        "            end = ls[1]\n",
        "            asp_cate_pola = ls[2]\n",
        "\n",
        "            labels.append([int(start),int(end), asp_cate_pola])\n",
        "        return np.array(sorted(labels,key = lambda x: x[0]))\n",
        "\n",
        "    def load(self,):\n",
        "        _len = len(self.data)\n",
        "        for i in range(_len):\n",
        "            x = self.data.iloc[i,0].strip() #text\n",
        "            span_labels = self.data.iloc[i,1] #label\n",
        "            span_labels = np.array(sorted(span_labels,key = lambda x: x[0]))\n",
        "\n",
        "            x,y = self.transform(x,span_labels)\n",
        "            self.X.append(convert_unicode(x))\n",
        "            self.y.append(y)\n",
        "\n",
        "        return self.X,self.y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDMKcS7wyZYG"
      },
      "outputs": [],
      "source": [
        "class AlignLabel():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  def Convert2LabelPosition(self,label):\n",
        "    labels_position = []\n",
        "    lst_first_pos = np.array([i if \"B-\" in v else 0 for i,v in enumerate(label)])\n",
        "    lst_first_pos = np.argwhere(lst_first_pos != 0).reshape(1,-1)[0]\n",
        "\n",
        "    for i in range(len(lst_first_pos)):\n",
        "      # if i reach last pos: label range should be (i,len(label))\n",
        "      last_pos = lst_first_pos[i]\n",
        "      if i == len(lst_first_pos) - 1:\n",
        "        for j in range(lst_first_pos[i],len(label)):\n",
        "          if \"I-\" in label[j]:\n",
        "            last_pos = j\n",
        "      else:\n",
        "        for j in range(lst_first_pos[i],lst_first_pos[i+1]):\n",
        "          if \"I-\" in label[j]:\n",
        "            last_pos = j\n",
        "      labels_position.append([lst_first_pos[i],last_pos,label[lst_first_pos[i]]])\n",
        "\n",
        "    return np.array(labels_position)\n",
        "\n",
        "  def segment_and_alignLabel(self,x,y,tokenizer, SEP):\n",
        "    def segment_and_addSEP(seg,ismid):\n",
        "      seg = tokenizer.tokenize(seg)\n",
        "      seg = [\" \".join(s) for s in seg]\n",
        "      seg = \" \".join(seg)\n",
        "      return seg\n",
        "\n",
        "\n",
        "    y_new = []\n",
        "    label = self.Convert2LabelPosition(y)\n",
        "    preprocess = TextNormalize()\n",
        "\n",
        "    if len(label) == 0:\n",
        "      x_temp = preprocess.normalize(x)\n",
        "      x_temp = segment_and_addSEP(x_temp,False)\n",
        "      x_temp = \" \".join(x_temp.split())\n",
        "      y_new = [\"O\" for i in range(len(x_temp.split()))]\n",
        "      return  [x_temp,np.array(y_new)]\n",
        "\n",
        "    first_index = label[:,0].astype(int)\n",
        "    second_index = label[:,1].astype(int)\n",
        "    asp_cate_pola = label[:,2]\n",
        "    s = 0\n",
        "    a = \"\"\n",
        "    x = x.split()\n",
        "    for i in range(len(label)):\n",
        "        front = \" \".join(x[s:first_index[i]])\n",
        "        if first_index[i] == second_index[i]:\n",
        "          middle = x[first_index[i]]\n",
        "          s = second_index[i] + 1\n",
        "        else:\n",
        "          middle = \" \".join(x[first_index[i]:second_index[i]+1])\n",
        "          s = second_index[i]+1\n",
        "\n",
        "        front = preprocess.normalize(front)\n",
        "        middle = preprocess.normalize(middle)\n",
        "\n",
        "        front = segment_and_addSEP(front,False)\n",
        "        middle = segment_and_addSEP(middle,True )\n",
        "\n",
        "        a += front + \" \" + middle + \" \"\n",
        "        y_new.extend([\"O\" for i in range(len(front.split()))])\n",
        "\n",
        "        if first_index[i] == second_index[i]:\n",
        "          y_new.extend([f\"B-{asp_cate_pola[i][2:]}\"])\n",
        "        else:\n",
        "          y_new.extend([f\"B-{asp_cate_pola[i][2:]}\" if j == 0 else f\"I-{asp_cate_pola[i][2:]}\" for j in range(len(middle.split(\" \")))])\n",
        "\n",
        "    if s != len(x):\n",
        "        enc = \" \".join(x[s:])\n",
        "        enc = preprocess.normalize(enc)\n",
        "        enc = segment_and_addSEP(enc,False)\n",
        "        a+= enc\n",
        "        y_new.extend([\"O\" for i in range(len(enc.split()))])\n",
        "\n",
        "    a = \" \".join(a.split())\n",
        "    return [a,np.array(y_new)]\n",
        "\n",
        "  def tokenize_and_alignlabel(self,x,y,tag2idx,tokenizer):\n",
        "    x = x.strip().split(\" \")\n",
        "    y_position = self.Convert2LabelPosition(y)\n",
        "    if len(y_position) == 0:\n",
        "      return np.zeros(MAX_LEN)\n",
        "\n",
        "    first_index = y_position[:,0].astype(int)\n",
        "    second_index = y_position[:,1].astype(int)\n",
        "    asp_cate_pola = y_position[:,2]\n",
        "    y_new = np.zeros(MAX_LEN)\n",
        "    x_tokenize = []\n",
        "    s = 0\n",
        "    pre_len = 0\n",
        "    y_position = 1\n",
        "    for i in range(len(first_index)):\n",
        "      front_len = len(tokenizer(\" \".join(x[s:first_index[i]]),add_special_tokens = False)['input_ids'])\n",
        "      y_new[y_position:y_position + front_len] = tag2idx[\"O\"]\n",
        "      y_position += front_len\n",
        "\n",
        "      if first_index[i] == second_index[i]:\n",
        "        words = tokenizer(x[first_index[i]],add_special_tokens = False)['input_ids']\n",
        "        s = second_index[i] + 1\n",
        "        # print(x[first_index[i]])\n",
        "\n",
        "      else:\n",
        "        # print(x[first_index[i]:second_index[i]+1],)\n",
        "        words = tokenizer(\" \".join(x[first_index[i]:second_index[i]+1]),add_special_tokens = False)['input_ids']\n",
        "        s = second_index[i] + 1\n",
        "      y_new[y_position] = tag2idx[f'B-{asp_cate_pola[i][2:]}']\n",
        "      # print(words)\n",
        "      if len(words) >= 2:\n",
        "        y_new[y_position+1:y_position+1+len(words)-1] = tag2idx[f'I-{asp_cate_pola[i][2:]}'] # skip B-name position, subtract -1 because len(words) contain B-name\n",
        "\n",
        "      y_position += len(words)\n",
        "\n",
        "    tokenize_last_sents = tokenizer(\" \".join(x[s:]),add_special_tokens = False)['input_ids']\n",
        "    y_new[y_position:y_position + len(tokenize_last_sents) + 1] = tag2idx[\"O\"]\n",
        "\n",
        "    end_sep_position = y_position  + len(tokenizer(\" \".join(x[s:])))\n",
        "\n",
        "    y_new[0] = tag2idx['O']\n",
        "    y_new[end_sep_position ] = tag2idx['O']\n",
        "\n",
        "    return y_new\n",
        "\n",
        "  def transform(self,x_raw,y_raw,rdrsegmenter,SEP):\n",
        "    X = []\n",
        "    Y = []\n",
        "    for i in range(len(x_raw)):\n",
        "      x, y = self.segment_and_alignLabel(x_raw[i],y_raw[i],rdrsegmenter,SEP)\n",
        "      X.append(x)\n",
        "      Y.append(y)\n",
        "    X = np.asarray(X)\n",
        "    Y = np.asarray(Y)\n",
        "    return X,Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eqhj7MKqOal"
      },
      "source": [
        "## READ DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mQToL2aWQNS"
      },
      "outputs": [],
      "source": [
        "data = LoadData(\"/content/drive/MyDrive/Nhóm - Tiến + Quý + Khanh + Văn/IE403 - Khai thác dữ liệu truyền thông xã hội/Đồ án/dataset/train.jsonl\")\n",
        "X_raw,y_raw = data.load()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50BTh_LCyn4n"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_dev, y_train, y_dev = train_test_split(X_raw, y_raw, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4xNfYY_yA89"
      },
      "outputs": [],
      "source": [
        "f= open('/content/drive/MyDrive/Đồ án KHDL/word2vec_vi_words_100dims.txt','r',encoding='utf-8')\n",
        "words = []\n",
        "embedding_words = {}\n",
        "\n",
        "i = 0\n",
        "for line in f:\n",
        "  if i == 0:\n",
        "    i+= 1\n",
        "    continue\n",
        "  value = line.split(' ')\n",
        "  word = value[0]\n",
        "  words.append(word)\n",
        "  try:\n",
        "    coefs = value[1:]\n",
        "    embedding_words[word] = np.asarray(coefs,dtype=np.float32)\n",
        "  except:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpiE46G5zZmy"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55kUaj3c1zjQ"
      },
      "outputs": [],
      "source": [
        "num_word = len(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRLn00iX2_WQ"
      },
      "outputs": [],
      "source": [
        "word2idx = {w:i for i,w in enumerate(words,start = 2)}\n",
        "word2idx['PAD'] = 0\n",
        "word2idx['UNK'] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28zEf0u1WQNX"
      },
      "outputs": [],
      "source": [
        "idx2word = {i:w for w,i in word2idx.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8cI1bjD1bsi"
      },
      "outputs": [],
      "source": [
        "embedding_matrix = np.ones((num_word,embedding_dim))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUXu4VzA1brv"
      },
      "outputs": [],
      "source": [
        "for w,i in word2idx.items():\n",
        "  if i > 10000:\n",
        "    continue\n",
        "  embedding_vector = embedding_words.get(w)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector\n",
        "  else:\n",
        "    embedding_matrix[i] = np.random.randn(100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBcroj08PfSe",
        "outputId": "5795a71f-8fd2-42ab-8dbb-8ec10caa6881"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 5.74881658e-02, -8.54510441e-02,  7.12853372e-02, ...,\n",
              "        -1.55041525e-02, -9.15896967e-02, -4.40163277e-02],\n",
              "       [-1.33968771e-01,  7.32150301e-02,  9.38539568e-04, ...,\n",
              "        -5.94867505e-02,  9.17428359e-02, -6.27832860e-02],\n",
              "       [-1.34450188e-02,  8.29119608e-02,  9.51128900e-02, ...,\n",
              "        -1.38094872e-01,  7.76888207e-02, -2.10761756e-01],\n",
              "       ...,\n",
              "       [ 1.00000000e+00,  1.00000000e+00,  1.00000000e+00, ...,\n",
              "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00],\n",
              "       [ 1.00000000e+00,  1.00000000e+00,  1.00000000e+00, ...,\n",
              "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00],\n",
              "       [ 1.00000000e+00,  1.00000000e+00,  1.00000000e+00, ...,\n",
              "         1.00000000e+00,  1.00000000e+00,  1.00000000e+00]])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJk7_p83tZD0"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 60\n",
        "def encoded(X):\n",
        "  x_new = []\n",
        "  for sent in X:\n",
        "    te = []\n",
        "    for word in sent.split():\n",
        "      try:\n",
        "        te.append(word2idx[word])\n",
        "      except:\n",
        "        te.append(word2idx[\"UNK\"])\n",
        "    x_new.append(te)\n",
        "  x_new = pad_sequences(x_new,maxlen=MAX_LEN,padding='post',truncating = 'post',value = word2idx['PAD'])\n",
        "  return x_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juzp91h0zFpQ"
      },
      "outputs": [],
      "source": [
        "def getTag2idx():\n",
        "  \"\"\"\n",
        "    This will return tag2idx, idx2tag\n",
        "  \"\"\"\n",
        "  aspect = np.array([\"SCREEN\",\"CAMERA\",\"FEATURES\",\"BATTERY\",\"PERFORMANCE\",\"STORAGE\",\"DESIGN\",\"PRICE\",\"GENERAL\",\"SER&ACC\"])\n",
        "  func_add_pola = lambda aspect,pola: [aspect[i] + \"#\" + pola for i in range(len(aspect))]\n",
        "  func_add_prefix = lambda aspect,prefix: [prefix + \"-\" + aspect[i] for i in range(len(aspect))]\n",
        "\n",
        "  aspect_pos = func_add_pola(aspect,\"POSITIVE\")\n",
        "  aspect_neu = func_add_pola(aspect,\"NEUTRAL\")\n",
        "  aspect_neg = func_add_pola(aspect,\"NEGATIVE\")\n",
        "\n",
        "  B_aspect_pos = func_add_prefix(aspect_pos,\"B\")\n",
        "  B_aspect_neu = func_add_prefix(aspect_neu,\"B\")\n",
        "  B_aspect_neg = func_add_prefix(aspect_neg,\"B\")\n",
        "\n",
        "  I_aspect_pos = func_add_prefix(aspect_pos,\"I\")\n",
        "  I_aspect_neu = func_add_prefix(aspect_neu,\"I\")\n",
        "  I_aspect_neg = func_add_prefix(aspect_neg,\"I\")\n",
        "  all_labels = np.concatenate([B_aspect_pos,B_aspect_neu,B_aspect_neg,I_aspect_pos,I_aspect_neu,I_aspect_neg])\n",
        "  tag2idx = {v:i+2 for i,v in enumerate(all_labels)}\n",
        "  tag2idx[\"O\"] = 1\n",
        "  tag2idx[\"PAD\"] = 0\n",
        "  idx2tag = {v:k for k,v in tag2idx.items()}\n",
        "  return tag2idx, idx2tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbS3Bo-8zG8v"
      },
      "outputs": [],
      "source": [
        "tag2idx, idx2tag = getTag2idx()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUyRhjK1t2z1"
      },
      "outputs": [],
      "source": [
        "X_train_encoded = encoded(X_train)\n",
        "X_dev_encoded = encoded(X_dev)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vY5LXZTzVz1",
        "outputId": "58a7914b-02c6-4b0a-c0bb-d2be76c5b93d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 3477,  6178,   742, ...,     0,     0,     0],\n",
              "       [ 4449,   256,     2, ...,   611,  1262,  1458],\n",
              "       [ 4449,   162,   131, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [ 4551,   186,   564, ...,     0,     0,     0],\n",
              "       [40912,    26,  7147, ...,     0,     0,     0],\n",
              "       [ 5869,     1,  4576, ...,     0,     0,     0]], dtype=int32)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybuKVeDdWQNZ"
      },
      "outputs": [],
      "source": [
        "y_train_encoded = [[tag2idx[w] for w in y] for y in y_train]\n",
        "y_train_encoded = pad_sequences(y_train_encoded,maxlen=MAX_LEN,padding='post',truncating = 'post',value = tag2idx['PAD'])\n",
        "y_train_encoded = [[to_categorical(z,num_classes = len(tag2idx)) for z in a] for a in y_train_encoded]\n",
        "\n",
        "y_dev_encoded = [[tag2idx[w] for w in y] for y in y_dev]\n",
        "y_dev_encoded = pad_sequences(y_dev_encoded,maxlen=MAX_LEN,padding='post',truncating = 'post',value = tag2idx['PAD'])\n",
        "y_dev_encoded = [[to_categorical(z,num_classes = len(tag2idx)) for z in a] for a in y_dev_encoded]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zBixE5NWQNZ"
      },
      "source": [
        "# MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je_2xCoSqgm0"
      },
      "source": [
        "## CRF CLASS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkBrL_2hzv7G",
        "outputId": "1a2f103b-8349-4313-d80c-d5f38761a57e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.20.0 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQvuwlyGg9rO",
        "outputId": "be7f4f4f-098b-4f97-e945-d8cb7f0f83f7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow.keras.layers as L\n",
        "from tensorflow_addons.text import crf_log_likelihood, crf_decode\n",
        "\n",
        "\n",
        "class CRF(L.Layer):\n",
        "    def __init__(self,\n",
        "                 output_dim,\n",
        "                 sparse_target=True,\n",
        "                 **kwargs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            output_dim (int): the number of labels to tag each temporal input.\n",
        "            sparse_target (bool): whether the the ground-truth label represented in one-hot.\n",
        "        Input shape:\n",
        "            (batch_size, sentence length, output_dim)\n",
        "        Output shape:\n",
        "            (batch_size, sentence length, output_dim)\n",
        "        \"\"\"\n",
        "        super(CRF, self).__init__(**kwargs)\n",
        "        self.output_dim = int(output_dim)\n",
        "        self.sparse_target = sparse_target\n",
        "        self.input_spec = L.InputSpec(min_ndim=3)\n",
        "        self.supports_masking = False\n",
        "        self.sequence_lengths = None\n",
        "        self.transitions = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "        f_shape = tf.TensorShape(input_shape)\n",
        "        input_spec = L.InputSpec(min_ndim=3, axes={-1: f_shape[-1]})\n",
        "\n",
        "        if f_shape[-1] is None:\n",
        "            raise ValueError('The last dimension of the inputs to `CRF` '\n",
        "                             'should be defined. Found `None`.')\n",
        "        if f_shape[-1] != self.output_dim:\n",
        "            raise ValueError('The last dimension of the input shape must be equal to output'\n",
        "                             ' shape. Use a linear layer if needed.')\n",
        "        self.input_spec = input_spec\n",
        "        self.transitions = self.add_weight(name='transitions',\n",
        "                                           shape=[self.output_dim, self.output_dim],\n",
        "                                           initializer='glorot_uniform',\n",
        "                                           trainable=True)\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        # Just pass the received mask from previous layer, to the next layer or\n",
        "        # manipulate it if this layer changes the shape of the input\n",
        "        return mask\n",
        "\n",
        "    def call(self, inputs, sequence_lengths=None, training=None, **kwargs):\n",
        "        sequences = tf.convert_to_tensor(inputs, dtype=self.dtype)\n",
        "        if sequence_lengths is not None:\n",
        "            assert len(sequence_lengths.shape) == 2\n",
        "            assert tf.convert_to_tensor(sequence_lengths).dtype == 'int32'\n",
        "            seq_len_shape = tf.convert_to_tensor(sequence_lengths).get_shape().as_list()\n",
        "            assert seq_len_shape[1] == 1\n",
        "            self.sequence_lengths = K.flatten(sequence_lengths)\n",
        "        else:\n",
        "            self.sequence_lengths = tf.ones(tf.shape(inputs)[0], dtype=tf.int32) * (\n",
        "                tf.shape(inputs)[1]\n",
        "            )\n",
        "\n",
        "        viterbi_sequence, _ = crf_decode(sequences,\n",
        "                                         self.transitions,\n",
        "                                         self.sequence_lengths)\n",
        "        output = K.one_hot(viterbi_sequence, self.output_dim)\n",
        "        return K.in_train_phase(sequences, output)\n",
        "\n",
        "    @property\n",
        "    def loss(self):\n",
        "        def crf_loss(y_true, y_pred):\n",
        "            y_pred = tf.convert_to_tensor(y_pred, dtype=self.dtype)\n",
        "            log_likelihood, self.transitions = crf_log_likelihood(\n",
        "                y_pred,\n",
        "                tf.cast(K.argmax(y_true), dtype=tf.int32) if self.sparse_target else y_true,\n",
        "                self.sequence_lengths,\n",
        "                transition_params=self.transitions,\n",
        "            )\n",
        "            return tf.reduce_mean(-log_likelihood)\n",
        "        return crf_loss\n",
        "\n",
        "    @property\n",
        "    def accuracy(self):\n",
        "        def viterbi_accuracy(y_true, y_pred):\n",
        "            # -1e10 to avoid zero at sum(mask)\n",
        "            mask = K.cast(\n",
        "                K.all(K.greater(y_pred, -1e10), axis=2), K.floatx())\n",
        "            shape = tf.shape(y_pred)\n",
        "            sequence_lengths = tf.ones(shape[0], dtype=tf.int32) * (shape[1])\n",
        "            y_pred, _ = crf_decode(y_pred, self.transitions, sequence_lengths)\n",
        "            if self.sparse_target:\n",
        "                y_true = K.argmax(y_true, 2)\n",
        "            y_pred = K.cast(y_pred, 'int32')\n",
        "            y_true = K.cast(y_true, 'int32')\n",
        "            corrects = K.cast(K.equal(y_true, y_pred), K.floatx())\n",
        "            return K.sum(corrects * mask) / K.sum(mask)\n",
        "        return viterbi_accuracy\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        tf.TensorShape(input_shape).assert_has_rank(3)\n",
        "        return input_shape[:2] + (self.output_dim,)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'output_dim': self.output_dim,\n",
        "            'sparse_target': self.sparse_target,\n",
        "            'supports_masking': self.supports_masking,\n",
        "            'transitions': K.eval(self.transitions)\n",
        "        }\n",
        "        base_config = super(CRF, self).get_config()\n",
        "        return dict(base_config, **config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3plUSvN_z2lu"
      },
      "outputs": [],
      "source": [
        "from keras.initializers import Constant\n",
        "from keras.layers import Dense,Input,GlobalAveragePooling1D ,concatenate,Dropout,GRU,Bidirectional,TimeDistributed, Embedding, Attention, LSTM,Convolution1D,MaxPooling1D,Flatten,SpatialDropout1D,LeakyReLU,AveragePooling1D,MultiHeadAttention,GlobalMaxPooling1D,Dropout\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adamax,Adam\n",
        "from keras.losses import CategoricalCrossentropy,BinaryCrossentropy\n",
        "from keras.regularizers import L1,L2\n",
        "# from tensorflow_addons.layers import CRF\n",
        "# from tensorflow_addons.losses import SigmoidFocalCrossEntropy\n",
        "from keras.initializers import Orthogonal\n",
        "from keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jutWc37rWQNa",
        "outputId": "bed63e1d-a554-4e8d-fab5-16cc3c8e4301"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 60)]              0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, 60, 100)           158750700 \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 60, 200)          160800    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_2 (TimeDis  (None, 60, 62)           12462     \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 158,923,962\n",
            "Trainable params: 158,923,962\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "input = Input(shape=(MAX_LEN,))\n",
        "embedding = Embedding(input_dim= len(word2idx), output_dim=100,\n",
        "                  input_length=MAX_LEN, embeddings_initializer = Constant(embedding_matrix))(input)\n",
        "bi_lstm = Bidirectional(LSTM(units=100, return_sequences=True,\n",
        "                           recurrent_dropout=0.1))(embedding)\n",
        "\n",
        "time = TimeDistributed(Dense(len(tag2idx), activation=\"softmax\"))(bi_lstm)\n",
        "\n",
        "w_model = Model(input,time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHtsAPimGDKN",
        "outputId": "ce6ce7db-690b-443c-b2f5-4b27afa6fa30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 60)]              0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, 60, 100)           158750700 \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 60, 200)          160800    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_2 (TimeDis  (None, 60, 62)           12462     \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 158,923,962\n",
            "Trainable params: 158,923,962\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "w_model.compile(optimizer=Adamax(learning_rate = 0.005),loss = 'categorical_crossentropy')\n",
        "w_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0axzRi4idET"
      },
      "outputs": [],
      "source": [
        "callback = EarlyStopping('val_loss',patience = 2,restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MM1GVzn10J1Y"
      },
      "outputs": [],
      "source": [
        "train = tf.data.Dataset.from_tensor_slices((X_train_encoded,y_train_encoded))\n",
        "train = train.batch(8).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "dev = tf.data.Dataset.from_tensor_slices((X_dev_encoded,y_dev_encoded))\n",
        "dev = dev.batch(8).cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfYu16Ve0-a5",
        "outputId": "162a1435-a71e-43bf-8f4a-d22a270228d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 60), dtype=tf.int32, name=None), TensorSpec(shape=(None, 60, 62), dtype=tf.float32, name=None))>"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "id": "hP2nhMjGxlQ1",
        "outputId": "a7135df4-e241-4215-c605-f09fd9dbc97e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "682/682 [==============================] - 358s 519ms/step - loss: 1.2565 - val_loss: 0.9104\n",
            "Epoch 2/50\n",
            "682/682 [==============================] - 352s 516ms/step - loss: 0.8263 - val_loss: 0.7604\n",
            "Epoch 3/50\n",
            "682/682 [==============================] - 368s 539ms/step - loss: 0.6948 - val_loss: 0.6884\n",
            "Epoch 4/50\n",
            "682/682 [==============================] - 348s 511ms/step - loss: 0.6203 - val_loss: 0.6553\n",
            "Epoch 5/50\n",
            "682/682 [==============================] - 348s 510ms/step - loss: 0.5658 - val_loss: 0.6446\n",
            "Epoch 6/50\n",
            "682/682 [==============================] - 353s 518ms/step - loss: 0.5247 - val_loss: 0.6317\n",
            "Epoch 7/50\n",
            "216/682 [========>.....................] - ETA: 3:59 - loss: 0.5037"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-105-f0d86b867d94>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m w_model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "w_model.fit(\n",
        "    train,\n",
        "    validation_data = dev,\n",
        "    epochs = 50,\n",
        "    callbacks = [callback],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhDC3xJZ_WWT"
      },
      "source": [
        "# EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJEy_PkH_Fly"
      },
      "outputs": [],
      "source": [
        "test_data = LoadData(\"/content/drive/MyDrive/Nhóm - Tiến + Quý + Khanh + Văn/IE403 - Khai thác dữ liệu truyền thông xã hội/Đồ án/dataset/test.jsonl\")\n",
        "X_test,y_test = test_data.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QA9wy3j2_IWN"
      },
      "outputs": [],
      "source": [
        "X_test_encoded = encoded(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnY6RLBa_YHI"
      },
      "outputs": [],
      "source": [
        "y_test_encoded = [[tag2idx[w] for w in y] for y in y_test]\n",
        "y_test_encoded = pad_sequences(y_test_encoded,maxlen=MAX_LEN,padding='post',truncating = 'post',value = tag2idx['PAD'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZZ4DbqEAB1e"
      },
      "outputs": [],
      "source": [
        "y_temp = y_test_encoded.reshape(1,-1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mX1YsreAeMF"
      },
      "outputs": [],
      "source": [
        "def pred2tag(y):\n",
        "  y = y.astype('object')\n",
        "  for row in range(y.shape[0]):\n",
        "    for col in range(y.shape[1]):\n",
        "      y[row][col] = idx2tag[y[row][col]]\n",
        "  return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfgi9MePAf-8"
      },
      "outputs": [],
      "source": [
        "true = pred2tag(y_test_encoded)\n",
        "tag = np.array(tag2idx.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMpcQx73A-yY"
      },
      "outputs": [],
      "source": [
        "aspect = np.array([\"SCREEN\",\"CAMERA\",\"FEATURES\",\"BATTERY\",\"PERFORMANCE\",\"STORAGE\",\"DESIGN\",\"PRICE\",\"GENERAL\",\"SER&ACC\"])\n",
        "func_add_pola = lambda aspect,pola: [aspect[i] + \"#\" + pola for i in range(len(aspect))]\n",
        "\n",
        "aspect_pos = func_add_pola(aspect,\"POSITIVE\")\n",
        "aspect_neu = func_add_pola(aspect,\"NEUTRAL\")\n",
        "aspect_neg = func_add_pola(aspect,\"NEGATIVE\")\n",
        "\n",
        "tags = np.concatenate([aspect_pos,aspect_neu,aspect_neg])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMGZYb1-O9vW"
      },
      "outputs": [],
      "source": [
        "test = tf.data.Dataset.from_tensor_slices((X_test_encoded,y_test_encoded))\n",
        "test = test.batch(8).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vrgZJkkJJcQ"
      },
      "source": [
        "## BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaVOEO2jO_02",
        "outputId": "7501e2b1-fdba-4544-9269-9f9fbeb23157"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "279/279 [==============================] - 22s 69ms/step\n"
          ]
        }
      ],
      "source": [
        "y_test_pred = np.argmax(w_model.predict(test),axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgkeHjJ0PBU2"
      },
      "outputs": [],
      "source": [
        "pred = pred2tag(y_test_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ON5KYJImPCoS",
        "outputId": "5db99326-f0bb-4da4-92d8-1f358adaac83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([['B-GENERAL#POSITIVE', 'I-GENERAL#POSITIVE', 'O', ..., 'PAD',\n",
              "        'PAD', 'PAD'],\n",
              "       ['O', 'O', 'O', ..., 'PAD', 'PAD', 'PAD'],\n",
              "       ['B-GENERAL#POSITIVE', 'I-GENERAL#POSITIVE', 'I-GENERAL#POSITIVE',\n",
              "        ..., 'PAD', 'PAD', 'PAD'],\n",
              "       ...,\n",
              "       ['O', 'O', 'O', ..., 'PAD', 'PAD', 'PAD'],\n",
              "       ['O', 'O', 'O', ..., 'O', 'O', 'I-SER&ACC#POSITIVE'],\n",
              "       ['B-PERFORMANCE#POSITIVE', 'I-PERFORMANCE#POSITIVE',\n",
              "        'I-PERFORMANCE#POSITIVE', ..., 'PAD', 'PAD', 'PAD']], dtype=object)"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpLJXvkaPD9n"
      },
      "outputs": [],
      "source": [
        "# https://github.com/MantisAI/nervaluate\n",
        "from nervaluate import Evaluator\n",
        "\n",
        "evaluator = Evaluator(true, pred, tags=tags, loader=\"list\")\n",
        "\n",
        "results, results_by_tag = evaluator.evaluate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTY2frXMPFw3",
        "outputId": "2e6b380a-62ec-40ca-a04b-70b7a1358851"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ent_type': {'correct': 4869,\n",
              "  'incorrect': 1472,\n",
              "  'partial': 0,\n",
              "  'missed': 480,\n",
              "  'spurious': 4164,\n",
              "  'possible': 6821,\n",
              "  'actual': 10505,\n",
              "  'precision': 0.4634935744883389,\n",
              "  'recall': 0.7138249523530275,\n",
              "  'f1': 0.5620454807803301},\n",
              " 'partial': {'correct': 3294,\n",
              "  'incorrect': 0,\n",
              "  'partial': 3047,\n",
              "  'missed': 480,\n",
              "  'spurious': 4164,\n",
              "  'possible': 6821,\n",
              "  'actual': 10505,\n",
              "  'precision': 0.45859114707282245,\n",
              "  'recall': 0.7062747397742266,\n",
              "  'f1': 0.55610065797068},\n",
              " 'strict': {'correct': 2988,\n",
              "  'incorrect': 3353,\n",
              "  'partial': 0,\n",
              "  'missed': 480,\n",
              "  'spurious': 4164,\n",
              "  'possible': 6821,\n",
              "  'actual': 10505,\n",
              "  'precision': 0.28443598286530225,\n",
              "  'recall': 0.4380589356399355,\n",
              "  'f1': 0.3449151564123283},\n",
              " 'exact': {'correct': 3294,\n",
              "  'incorrect': 3047,\n",
              "  'partial': 0,\n",
              "  'missed': 480,\n",
              "  'spurious': 4164,\n",
              "  'possible': 6821,\n",
              "  'actual': 10505,\n",
              "  'precision': 0.31356496906235126,\n",
              "  'recall': 0.48292039290426625,\n",
              "  'f1': 0.3802377929123861}}"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buVi1KxYJHYO"
      },
      "source": [
        "## BiLSTM-CRF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Z7YuCd6_mAD",
        "outputId": "902d5572-eb7c-4971-d5e8-b3d59fa19c29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "279/279 [==============================] - 26s 94ms/step\n"
          ]
        }
      ],
      "source": [
        "y_test_pred = np.argmax(w_model.predict(test),axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENmgbewc_6Yy"
      },
      "outputs": [],
      "source": [
        "pred = pred2tag(y_test_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxE80GWiBKNq",
        "outputId": "57460d8a-59c9-4626-f4c5-9a26344c9815"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([['O', 'O', 'O', ..., 'PAD', 'PAD', 'PAD'],\n",
              "       ['I-PERFORMANCE#NEGATIVE', 'I-PERFORMANCE#NEGATIVE',\n",
              "        'I-PERFORMANCE#NEGATIVE', ..., 'PAD', 'PAD', 'PAD'],\n",
              "       ['I-PERFORMANCE#NEGATIVE', 'I-PERFORMANCE#NEGATIVE',\n",
              "        'I-PERFORMANCE#NEGATIVE', ..., 'PAD', 'PAD', 'PAD'],\n",
              "       ...,\n",
              "       ['I-PERFORMANCE#NEGATIVE', 'I-PERFORMANCE#NEGATIVE',\n",
              "        'I-PERFORMANCE#NEGATIVE', ..., 'PAD', 'PAD', 'PAD'],\n",
              "       ['I-PERFORMANCE#NEGATIVE', 'I-PERFORMANCE#NEGATIVE',\n",
              "        'I-PERFORMANCE#NEGATIVE', ..., 'I-PERFORMANCE#NEGATIVE',\n",
              "        'I-PERFORMANCE#NEGATIVE', 'I-PERFORMANCE#NEGATIVE'],\n",
              "       ['I-PERFORMANCE#NEGATIVE', 'I-PERFORMANCE#NEGATIVE',\n",
              "        'I-PERFORMANCE#NEGATIVE', ..., 'PAD', 'PAD', 'PAD']], dtype=object)"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Jtoi17yAwke",
        "outputId": "3848ab78-f3bd-4a6c-d717-7533bd983ed3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nervaluate\n",
            "  Downloading nervaluate-0.1.8-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: nervaluate\n",
            "Successfully installed nervaluate-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!pip install nervaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8d1tqs4At9m"
      },
      "outputs": [],
      "source": [
        "# https://github.com/MantisAI/nervaluate\n",
        "from nervaluate import Evaluator\n",
        "\n",
        "evaluator = Evaluator(true, pred, tags=tags, loader=\"list\")\n",
        "\n",
        "results, results_by_tag = evaluator.evaluate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWSagybMBE2c",
        "outputId": "1fafbf62-f689-433b-832f-f2134d0392dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'correct': 8,\n",
              " 'incorrect': 0,\n",
              " 'partial': 1640,\n",
              " 'missed': 5173,\n",
              " 'spurious': 26,\n",
              " 'possible': 6821,\n",
              " 'actual': 1674,\n",
              " 'precision': 0.4946236559139785,\n",
              " 'recall': 0.12138982553877731,\n",
              " 'f1': 0.19493819894055325}"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results['strict']"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "6bfd7e43db9e4734bb66d5adfb403b43705154d42fa8cc6fabe3ef8413fb5b5c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
